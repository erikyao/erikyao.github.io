---
layout: post
title: "PR Curve"
description: ""
category: Machine-Learning
tags: [ROC, PRC]
---
{% include JB/setup %}

## 1. Axes

- x-axis 是 $\text{Recall} = \frac{TP}{TP+FN} = \frac{TP}{P}$ (i.e. $\text{True Positive Rate}$)
    - 没错，和 ROC 的 y-axis 一样
- y-axis 是 $\text{Precision} = \frac{TP}{TP+FP}$ (a.k.a. $\text{Positive Predictive Value}$)

## 2. 序列规律

```python
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_recall_curve

bc = load_breast_cancer()

feat = pd.DataFrame(bc['data'])
feat.columns = bc['feature_names']
label = pd.Series(bc['target'])

train_X = feat.loc[0:99, ]  # contrary to usual python slices, both the start and the stop are included in .loc!
train_y = label.loc[0:99]
test_X = feat.loc[100:, ]
test_y = label.loc[100:]

lr_config = dict(penalty='l2', C=1.0, class_weight=None, random_state=1337,
                 solver='liblinear', max_iter=100, verbose=0, warm_start=False, n_jobs=1)
lr = LogisticRegression(**lr_config)
lr.fit(train_X, train_y)

proba_test_y = lr.predict_proba(test_X)[:, 1]

precision, recall, threshold = precision_recall_curve(test_y, proba_test_y, pos_label=1)
```

这里有一点需要注意：`precision`, `recall`, `threshold` 这三个 array 的长度是不一样的！

```python
>>> print(len(precision), len(recall), len(threshold))
376 376 375
```

从 [源代码](https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/metrics/ranking.py#L450) 的最后一行来看：

```python
def precision_recall_curve(y_true, probas_pred, pos_label=None, sample_weight=None):
    fps, tps, thresholds = _binary_clf_curve(y_true, probas_pred,
                                             pos_label=pos_label,
                                             sample_weight=sample_weight)

    precision = tps / (tps + fps)
    recall = tps / tps[-1]

    # stop when full recall attained
    # and reverse the outputs so recall is decreasing
    last_ind = tps.searchsorted(tps[-1])
    sl = slice(last_ind, None, -1)
    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]
```

- `(recall=0, precision=1)` 这个点完全是人为添加上去的，但是没有添加对应的 `threshold` 的值
    - 这里 `np.r_` 你姑且理解为 concat 就好了

这里有几个点需要搞清楚：

- 假设 `len(recall) == len(precision) == n`，那么：`recall[0:n-2]`、`precision[0:n-2]` 与 `threshold[0:n-2]` 是一一对应的
- `threshold` 是递增的，从 0 到非常接近 1
- `recall` 是随着 `threshold` 递增而单调递减的，这在 [ROC Curve: my interpretation](/machine-learning/2018/02/26/roc-curve-my-interpretation) 里就已经讨论过了
- `precision` 并没有随着 `threshold` 递增而单调递减，但总体是下降趋势

所以 `(recall=0, precision=1)` 这个点其实是 PR curve 上的第一个点，直觉上它应该对应 `threshold=1`，但是严格按照定义来说：

- `threshold=1` 时，没有 prediction 为 positive，所以 $TP$ 和 $FP$ 更是无从谈起，全都是 0，进而 $\text{Precision} =\frac{TP}{TP+FP} = \frac{0}{0}$，undefined

我觉得 scikit-learn 这么处理完全是为了画图上的方便。像 [Introduction to the precision-recall plot](https://classeval.wordpress.com/introduction/introduction-to-the-precision-recall-plot/) 里提到的 "Estimating the first point from the second point" 方法也是很有道理的，但是明显 scikit-learn 采取了 "直接设定第一个点为 `(recall=0, precision=1)`" 这么一个简单粗暴的做法。

为了方便做示例，我们也人为添加一个 `threshold=1`:

```python
thresholds = np.r_[threshold, 1]

df = pd.DataFrame({"Precision": precision, "Recall": recall, "Decision Threshold":threshold})
df.to_csv("auprc_test.tsv", sep='\t', index=False, header=True)
```

```r
>>> library(ggplot2)
>>> auprc_df <- read.table("auprc_test.tsv", header=TRUE, sep="\t", stringsAsFactors=FALSE)
>>> colnames(auprc_df)
'Decision.Threshold' 'Precision' 'Recall'
>>> p <- ggplot(data=auprc_df, mapping=aes(x=Recall, y=Precision)) + 
         geom_path(size=0.3) + geom_point(size=0.4, color=I("blue"))
>>> p
```

这里用 `geom_path` 而不是 `geom_line` 的原因在 [ggplot2: use geom_line() carefully when your x-axis data are descending](/r/2018/03/15/ggplot2-use-geom_line-carefully-when-your-x-axis-data-are-descending) 里有说明。

![][prc]

标上 `theshold` 数据看看：

```r
>>> p <- ggplot(data=auprc_df[40:60,], mapping=aes(x=Recall, y=Precision)) + 
         geom_path(size=0.3) + geom_point(size=0.4, color=I("blue"))
>>> p + geom_text(aes(label=sprintf("%0.3f", round(Decision.Threshold, digits = 3))), hjust=-0.2, vjust=-0.4, size=2.2)
```

![][prc_with_thld]

## 3. 为什么 `precision` 没有单调性

当 threshold 从 $t_j$ 变化到 $t_{j+1}$ 时：

- $t_j > t_{j+1}$
- $\lbrace h(x_i) = 1 \rbrace \subseteq \lbrace h(x_{i+1}) = 1 \rbrace $ 
    - threshold 减小，predict 为 positive 的数量只多不少
    - 尤其当 $t = 0$ 时，$\vert \lbrace h(x_i) = 1 \rbrace \vert = n$
- 对 $\lbrace \text{Precision}_j \rbrace$ 而言:
    - $FP \rvert_{t_j} \leq FP \rvert_{t_{j+1}}$
        - $\lbrace h(x_{i+1}) = 1 \rbrace \setminus \lbrace h(x_i) = 1 \rbrace$ 是新增的 predict 为 positive 的 cases，其中必然有 0 个或者若干个新增的 False Positive
    - $TP \rvert_{t_j} \leq TP \rvert_{t_{j+1}}$ 
        - $\lbrace h(x_{i+1}) = 1 \rbrace \setminus \lbrace h(x_i) = 1 \rbrace$ 是新增的 predict 为 positive 的 cases，其中必然有 0 个或者若干个新增的 True Positive
    - 但无法保证有 $\frac{TP \rvert_{t_j}}{TP\rvert_{t_j} + FP \rvert_{t_j}} \leq \frac{TP \rvert_{t_{j+1}}}{TP \rvert_{t_{j+1}} + FP \rvert_{t_{j+1}}}$

## 4. Baseline

当 threshold 为 0 时，所有的 prediction 都是 positive，从而 $TP = P, FP = N$，进而 $\text{Precision} =\frac{TP}{TP+FP} = \frac{P}{P+N} = \text{class balance}$。

我们接着用 [ROC Curve: my interpretation](/machine-learning/2018/02/26/roc-curve-my-interpretation) 里 random guess 的例子：

> Similarly, if you predict a random assortment of 0’s and 1’s, let’s say 90% 1’s.

- 你在 $P$ 上预测了 90% 为 Positive，这些全部都是 True Positive，所以 $TP = 0.9 * P$
- 你在 $N$ 上预测了 90% 为 Positive，这些全部都是 False Positive，所以 $FP = 0.9 * N$
- 你在 $P$ 上预测了 10% 为 Negative，这些全部都是 False Negative，所以 $FN = 0.1 * P$
- $\therefore \text{Recall} = \frac{TP}{P} = 0.9, \text{Precision} = \frac{TP}{TP+FP} = \frac{P}{P+N} = \text{class balance}$

所以你会得到一个点 $(recall=0.9, precision=class_balance)$。无数这样的 random guessing predictor 就构成了一条 baseline $y=\text{class balance}$。

考虑到 `precision` 并没有单调递减，所以可能出现 `precision` below baseline 的情况。

如果 AUPRC > baseline，说明我们的 predictor 好过 random guess。

最后，并没有 "baseline 以上 AUPRC" 这种指标。

## 5. PRC is sensitive to class balance

换言之，PRC 更适用于 imbalanced data。还是用 [ROC Curve: my interpretation](/machine-learning/2018/02/26/roc-curve-my-interpretation) 里的例子：

> - Balanced dataset: $P=1000, N=1000$; classifier $A$ predicts at threashold $t$: $TP=500, FP=160$  
> - Imbalanced dataset: $P=1000, N=10000$; classifier $B$ predicts at the same threashold $t$: $TP=500, FP=1600$  
> <br/>
> 如果在所有的 theashold 上都有这样类似的关系，i.e. $TP_A = TP_B, FP_A = \frac{1}{10} FP_B$，那么这两个 classifiers 的 ROC 是完全一样的

在这种设定下：

- $\text{Recall}_A = \text{Recall}_B = \frac{500}{1000} = 0.5$
- $\text{Precision}_A = \frac{500}{500+160} = 0.7576, \, \text{baseline}_A = \frac{1000}{1000+1000} = 0.5$
- $\text{Precision}_B = \frac{500}{500+1600} = 0.2381, \, \text{baseline}_B = \frac{1000}{1000+10000} = 0.091$

可以预见会有 $\text{AUPRC}_A > \text{AUPRC}_B$。

从定义上我们也可以看出，**PRC 强调的是 ”$TP$ 在所有我所做的 prediction 中的比例"**。

## 6. scikit-learn 如何计算 AUPRC

待续
---
layout: post
title: "Tensor"
description: ""
category: Math
tags: []
---
{% include JB/setup %}

## 1. 预备知识：covector

假定 $V$ is a vector space over a field $K$。以下四个名字：

- linear functional
- linear form
- one-form
- covector

指的是同一个对象：a function $f: V \to K$ which satisfies linearity:

- $f(\mathbf{v} + \mathbf{w}) = f(\mathbf{v}) + f(\mathbf{w}), \forall \mathbf{v}, \mathbf{w} \in V$
- $f(a\mathbf{v}) = a f(\mathbf{v}), \forall \mathbf{v} \in V, \forall a \in K$

一般这个 field $K$ 就是 $\mathbb{R}$，所以 covector 就是一个函数，它接收一个 (column) vector 作为参数，返回一个实数。

但本质上，我们也可以把 covector 理解为一个 (row) vector，因为我们可以定义 $f(\mathbf{v}) = \mathbf{w}^T \mathbf{v} = a \in \mathbb{R}$。进一步，我们集齐所有这样的函数 $f$，得到集合 $Hom_K(V,K) = \lbrace f: V \to K \mid f \text{ is linear} \rbrace$，这个集合可以构成一个 vector space over $K$ with operations of addition and scalar multiplication。我们称这个 vector space 为 $V^{\ast}$，它是 vector space $V$ 的 (algebraic) dual space。

注：

- $Hom$ 的意思是 "homomorphism", a transformation of one set, say $A$, into another, say $A'$, that the relations between elements of $A$ are preserved in $A'$.
- 换个角度考虑，vector 可以看做一个 $g: K \to V$，但是研究这个似乎没有意义

## 2. 预备知识：Einstein summation convention

其实就是个简写。比如把 $y=\sum_{i=1}^{3} c_{i}x^{i} = c_{1}x^{1}+c_{2}x^{2}+c_{3}x^{3}$ 简写成 $y=c_{i}x^{i}$。注意这里 $x^i$ 不是表示 "$i$ 次方" 而是 "第 $i$ 维"。

应用到 vector 的场合，假设有 $\mathbf{v} \in \mathbb{R}^{n}$，我们一般展开是：

$$
\mathbf{v} = \sum_{i=1}^{n} v_i \mathbf{e}_i
$$

其中 $v_i$ 是分量，$\mathbf{e}_i$ 是 basis (基)，即：

- $\mathbf{e}_1 = [1 \, 0 \, 0 \, \dots \, 0]^T \in \mathbb{R}^n$
- $\mathbf{e}_2 = [0 \, 1 \, 0 \, \dots \, 0]^T \in \mathbb{R}^n$
- $\dots$
- $\mathbf{e}_n = [0 \, 0 \, 0 \, \dots \, 1]^T \in \mathbb{R}^n$

用 Einstein summation convention 就可以简写成:

$$
\mathbf{v} = v_i \mathbf{e}_i
$$

为了以示区别，covector $\mathbf{w}^T$ 我们一般写成：

$$
\mathbf{w}^T = w^i \mathbf{e}^i
$$

其中：

- $\mathbf{e}^1 = [1 \, 0 \, 0 \, \dots \, 0]$
- $\mathbf{e}^2 = [0 \, 1 \, 0 \, \dots \, 0]$
- $\dots$
- $\mathbf{e}^n = [0 \, 0 \, 0 \, \dots \, 1]$

## 3. 预备知识：Banach space / Vector space from continuous $k$-linear maps

[Wikipedia: Banach space](https://en.wikipedia.org/wiki/Banach_space)

> ... a **Banach space** (pronounced [ˈbanax]) is a complete normed vector space. Thus, a Banach space is a vector space with a metric that allows the computation of vector length and distance between vectors and is complete in the sense that a Cauchy sequence of vectors always converges to a well defined limit that is within the space.

Let $V, K, \dots$ denote Banach spaces. We define $L^n(V_1, \dots, V_n; K)$ denotes the vector space of continuous $n$-linear maps of $V_1 \times \dots \times V_n \to K$.

注意：

- 这里 $V_i \times V_j$ 的 $\times$ 是 cartesian product，也就是说：$f: V_1 \times \dots \times V_n \to K$ 是一个函数，它接收一个 $n$-tuple of vectors，返回一个 $K$ 的元素
    - 即 $f(\mathbf{v}_1, \dots, \mathbf{v}_n) = k$ 其中 $\mathbf{v}_i \in V_i, k \in K$
- $f: V_1 \times \dots \times V_n \to \mathbb{R}$ 可以理解成一个 $n$-tuple of covectors，比如 $(\mathbf{w}\_1^T, \dots, \mathbf{w}\_n^T)$，因为我们可以定义 $f(\mathbf{v}\_1, \dots, \mathbf{v}\_n) = \prod\_{i=1}^{n} \mathbf{w}\_i^T \mathbf{v}\_i = a \in \mathbb{R}$。
- 这么一来，$L^n(V_1, \dots, V_n; \mathbb{R})$ 其实是一个 "元素为 $n$-tuple of covectors" 的 space。但是你结合 [Digest of _Essence of Linear Algebra_](/math/2016/11/17/digest-of-essence-of-linear-algebra) 最后的部分，"$n$-tuple of covectors" 满足 vector addition and scaling 的 8 条 axioms，所以可以看做一个 generalized 的 vector；换言之，$L^n(V_1, \dots, V_n; \mathbb{R})$ 也就是一个 generalized 的 vector space

## 4. Tensor Space / Rank

For a vector space $V$, we define:

$$
T_{s}^{r}(V) = L^{r+s}(\underbrace{V^{\ast}, \dots, V^{\ast}}_{r}, \underbrace{V, \dots, V}_{s}; \mathbb{R})
$$

Elements of $T_{s}^{r}(V)$ are called **tensors** on $V$, contravariant of order $r$ and covariant of order $s$; or simply, of type $(r, s)$.

Special cases:

- $T^0_0(V) = \mathbb{R}$
- $T^1_0(V) = L(V^{\ast}; \mathbb{R}) = V$
- $T^2_0(V) = L(V^{\ast}, V^{\ast}; \mathbb{R}) = L(V^{\ast}; V)$
- $T^0_1(V) = L(V; \mathbb{R}) = V^{\ast}$
- $T^0_2(V) = L(V, V; \mathbb{R}) = L(V; V^{\ast})$
- $T^1_1(V) = L(V^{\ast}, V; \mathbb{R}) = L(V; V) = L(V^{\ast}; V^{\ast})$

注意：$s+r$ 的值称作 tensor 的 rank；从 special case 来看：

- 0 阶 tensor 是 scalar
- 1 阶 tensor 是 vector/covector
- 2 阶 tensor 中只有 $T^1_1(V)$ 是 matrix
    - 即你只能说 matrix 是 2 阶 tensor；不能说 2 阶 tensor 都是 matrix
    - 仔细考虑一下，其实所有 $2n$ 阶的 $T^n_n(V)$ 都是 matrix
        - 当然这里说的都是 2-D matrix
            - 按 [Wikipedia: Tensor product of linear maps](https://en.wikipedia.org/wiki/Tensor_product#Tensor_product_of_linear_maps) 的例子，matrix 对应 tensor，matrix 的 Kronecker Product 对应 tensor product。两个 $2 \times 2$ matrix 的 Kronecker Product 是一个 $4 \times 4$ matrix，按道理它应该是一个 $T^2_2$
        - 你应该抽象地把 tensor 想象成 matrix of matrices；2-D matrix 只是 tensor 的特殊情况
            - 如果说 vector 是 1-D matrix，那么 tensor 还可以是 vector of vectors、vector of matrices、matrix of vectors，它们都是 matrix of matrices 的变种

## 5. Tensor

从 tensor space 来看，一个 tensor 就是个 $f: \underbrace{V^{\ast} \times \dots \times V^{\ast}}\_{r} \times \underbrace{V \times \dots \times V}\_{s} \to \mathbb{R}$

- 从函数的角度来看，$V^{\ast}$ 和 $V$ 的顺序其实是可以打乱的，也可以是交错的；但为了研究起来方便，tensor 的定义强制要求了这个 "连续 $V^{\ast}$ 再连续 $V$" 的顺序

这个 $f$ 可以看做一个 $(r+s)$-tuple of vectors/covectors $(\mathbf{p}_1, \dots, \mathbf{p}_r, \mathbf{q}_1^T, \dots, \mathbf{q}_s^T)$，因为我们可以定义：

$$
f(\mathbf{w}_1^T, \dots, \mathbf{w}_r^T, \mathbf{v}_1, \dots, \mathbf{v}_s) = \prod_{i=1}^{r} \mathbf{w}_i^T \mathbf{p}_i \times \prod_{i=1}^{s} \mathbf{q}_i^T \mathbf{v}_i = a \in \mathbb{R}
$$

## 6. Tensor Product Operator

为了表示起来方便，我们引入 tensor product operator $\otimes$。它其实有两种应用场合：

1. 两个 tensor space over vector space $V$, $\Theta_1$ 和 $\Theta_2$ 的 tensor product $\Theta_1 \otimes \Theta_2$ 仍然是一个 tensor over vector space $V$
    - 考虑特殊情况：假设 $V$ 和 $W$ 都是 vector space (i.e. $T^1_0(V)$) over field $K$，那么 $V \otimes_K W$ 仍然是一个 vector space over field $K$
2. 两个 tensor, $t_1$ 和 $t_2$ 的 tensor product $t_1 \otimes t_2$ 仍然是一个 tensor
    - 考虑特殊情况：vector/covector/matrix 之间也可以有 $\otimes$ 操作

我觉得暂时不要关注计算细节，先掌握大的计算原则比较重要。

### 6.1 用 $\otimes$ 表示 $L$

这个逻辑其实要绕一下：

- 假定有 $f: \underbrace{V^{\ast} \times \dots \times V^{\ast}}\_{r} \times \underbrace{V \times \dots \times V}\_{s} \to \mathbb{R}$，则 $f \in T\_{s}^{r}(V) = L^{r+s}(\underbrace{V^{\ast}, \dots, V^{\ast}}\_{r}, \underbrace{V, \dots, V}\_{s}; \mathbb{R})$
- 又:$f$ 可以看做一个 $(r+s)$-tuple of vectors/covectors $(\mathbf{p}_1, \dots, \mathbf{p}_r, \mathbf{q}_1^T, \dots, \mathbf{q}_s^T)$
- 我们可以写 $T\_{s}^{r}(V) = \underbrace{V \otimes \dots \otimes V}\_{r} \times \underbrace{V^{\ast} \otimes \dots \otimes V^{\ast}}\_{s}$
    - 注意这里 $V$、$V^{\ast}$ 的顺序和 $L$ 里是反的、和 $(r+s)$-tuple 是一致的

### 6.2 Rank of Tensor Product

基本原则：

$$
T_{s}^{r}(V) \otimes T_{s'}^{r'}(V) \to T_{s+s'}^{r+r'}(V)
$$

### 6.3 $T^0_0(V) = \mathbb{R}$ 的特殊性

严格来说，如果 $f_1: V \to X, f_2: W \to Y$，那么：

$$
f_1 \otimes f_2: V \otimes W \to X \otimes Y
$$

从函数的角度来看：

$$
(f_1 \otimes f_2)(\mathbf{v} \otimes \mathbf{w}) = f_1(\mathbf{v}) \otimes f_2(\mathbf{w})
$$

但是因为我们一般处理 $\mathbb{R}$，而 $\mathbb{R}$ 又是 $T^0_0(V)$，所以 $\mathbb{R} \otimes \mathbb{R} = T\_{0+0}^{0+0}(V) = \mathbb{R}$

所以我们一般的 $V \otimes W$ 的元素仍然是一个 $f: V \otimes W \to \mathbb{R}$
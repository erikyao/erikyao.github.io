---
layout: post
title: "Tensor"
description: ""
category: Math
tags: []
---
{% include JB/setup %}

参考：

- [什么是张量 (tensor)？ - 玟清的回答 - 知乎](https://www.zhihu.com/question/20695804/answer/43301304)

## 1. 预备知识：covector

假定 $V$ is a vector space over a field $K$。以下四个名字：

- linear functional
- linear form
- one-form
- covector

指的是同一个对象：a function $f: V \to K$ which satisfies linearity:

- $f(\mathbf{v} + \mathbf{w}) = f(\mathbf{v}) + f(\mathbf{w}), \forall \mathbf{v}, \mathbf{w} \in V$
- $f(a\mathbf{v}) = a f(\mathbf{v}), \forall \mathbf{v} \in V, \forall a \in K$

一般这个 field $K$ 就是 $\mathbb{R}$，所以 covector 就是一个函数，它接收一个 (column) vector 作为参数，返回一个实数。

但本质上，我们也可以把 covector 理解为一个 (row) vector，因为我们可以定义 $f(\mathbf{v}) = \mathbf{w}^T \mathbf{v} = a \in \mathbb{R}$。进一步，我们集齐所有这样的函数 $f$，得到集合 $Hom_K(V,K) = \lbrace f: V \to K \mid f \text{ is linear} \rbrace$，这个集合可以构成一个 vector space over $K$ with operations of addition and scalar multiplication。我们称这个 vector space 为 $V^{\ast}$，它是 vector space $V$ 的 (algebraic) dual space。

注：

- $Hom$ 的意思是 "homomorphism", a transformation of one set, say $A$, into another, say $A'$, that the relations between elements of $A$ are preserved in $A'$.
- 换个角度考虑，vector 可以看做一个 $g: K \to V$，但是研究这个似乎没有意义

## 2. 预备知识：Einstein summation convention

其实就是个简写。比如把 $y=\sum_{i=1}^{3} c_{i}x^{i} = c_{1}x^{1}+c_{2}x^{2}+c_{3}x^{3}$ 简写成 $y=c_{i}x^{i}$。注意这里 $x^i$ 不是表示 "$i$ 次方" 而是 "第 $i$ 维"。

应用到 vector 的场合，假设有 $\mathbf{v} \in \mathbb{R}^{n}$，我们一般展开是：

$$
\mathbf{v} = \sum_{i=1}^{n} v_i \mathbf{e}_i
$$

其中 $v_i$ 是分量，$\mathbf{e}_i$ 是 basis (基)，即：

- $\mathbf{e}_1 = [1 \, 0 \, 0 \, \dots \, 0]^T \in \mathbb{R}^n$
- $\mathbf{e}_2 = [0 \, 1 \, 0 \, \dots \, 0]^T \in \mathbb{R}^n$
- $\dots$
- $\mathbf{e}_n = [0 \, 0 \, 0 \, \dots \, 1]^T \in \mathbb{R}^n$

用 Einstein summation convention 就可以简写成:

$$
\mathbf{v} = v_i \mathbf{e}_i
$$

为了以示区别，covector $\mathbf{w}^T$ 我们一般写成：

$$
\mathbf{w}^T = w^i \mathbf{e}^i
$$

其中：

- $\mathbf{e}^1 = [1 \, 0 \, 0 \, \dots \, 0]$
- $\mathbf{e}^2 = [0 \, 1 \, 0 \, \dots \, 0]$
- $\dots$
- $\mathbf{e}^n = [0 \, 0 \, 0 \, \dots \, 1]$

## 3. 预备知识：Banach space / Vector space from continuous $k$-linear maps

[Wikipedia: Banach space](https://en.wikipedia.org/wiki/Banach_space)

> ... a **Banach space** (pronounced [ˈbanax]) is a complete normed vector space. Thus, a Banach space is a vector space with a metric that allows the computation of vector length and distance between vectors and is complete in the sense that a Cauchy sequence of vectors always converges to a well defined limit that is within the space.

Let $V, K, \dots$ denote Banach spaces. We define $L^n(V_1, \dots, V_n; K)$ denotes the vector space of continuous $n$-linear maps of $V_1 \times \dots \times V_n \to K$.

注意：

- 这里 $V_i \times V_j$ 的 $\times$ 是 cartesian product，也就是说：$f: V_1 \times \dots \times V_n \to K$ 是一个函数，它接收一个 $n$-tuple of vectors，返回一个 $K$ 的元素
    - 即 $f(\mathbf{v}_1, \dots, \mathbf{v}_n) = k$ 其中 $\mathbf{v}_i \in V_i, k \in K$
- $f: V_1 \times \dots \times V_n \to \mathbb{R}$ 可以理解成一个 $n$-tuple of covectors，比如 $(\mathbf{w}\_1^T, \dots, \mathbf{w}\_n^T)$，因为我们可以定义 $f(\mathbf{v}\_1, \dots, \mathbf{v}\_n) = \prod\_{i=1}^{n} \mathbf{w}\_i^T \mathbf{v}\_i = a \in \mathbb{R}$。
- 这么一来，$L^n(V_1, \dots, V_n; \mathbb{R})$ 其实是一个 "元素为 $n$-tuple of covectors" 的 space。但是你结合 [Digest of _Essence of Linear Algebra_](/math/2016/11/17/digest-of-essence-of-linear-algebra) 最后的部分，"$n$-tuple of covectors" 满足 vector addition and scaling 的 8 条 axioms，所以可以看做一个 generalized 的 vector；换言之，$L^n(V_1, \dots, V_n; \mathbb{R})$ 也就是一个 generalized 的 vector space

## 4. Tensor Space / Rank

For a vector space $V$, we define:

$$
T_{s}^{r}(V) = L^{r+s}(\underbrace{V^{\ast}, \dots, V^{\ast}}_{r}, \underbrace{V, \dots, V}_{s}; \mathbb{R})
$$

Elements of $T_{s}^{r}(V)$ are called **tensors** on $V$, contravariant of order $r$ and covariant of order $s$; or simply, of type $(r, s)$.

Special cases:

- $T^0_0(V) = \mathbb{R}$
- $T^1_0(V) = L(V^{\ast}; \mathbb{R}) = V$
- $T^2_0(V) = L(V^{\ast}, V^{\ast}; \mathbb{R}) = L(V^{\ast}; V)$
- $T^0_1(V) = L(V; \mathbb{R}) = V^{\ast}$
- $T^0_2(V) = L(V, V; \mathbb{R}) = L(V; V^{\ast})$
- $T^1_1(V) = L(V^{\ast}, V; \mathbb{R}) = L(V; V) = L(V^{\ast}; V^{\ast})$

注意：$s+r$ 的值称作 tensor 的 rank；从 special case 来看：

- 0 阶 tensor 是 scalar
- 1 阶 tensor 是 vector/covector
- 2 阶 tensor 中只有 $T^1_1(V)$ 是 matrix
    - 即你只能说 matrix 是 2 阶 tensor；不能说 2 阶 tensor 都是 matrix
    - 仔细考虑一下，其实所有 $2n$ 阶的 $T^n_n(V)$ 都是 matrix，见 6.2 的讨论
        - 当然这里说的都是 2-D 的实数 matrix

## 5. Tensor

从 tensor space 来看，一个 tensor 就是个 $f: \underbrace{V^{\ast} \times \dots \times V^{\ast}}\_{r} \times \underbrace{V \times \dots \times V}\_{s} \to \mathbb{R}$

- 从函数的角度来看，$V^{\ast}$ 和 $V$ 的顺序其实是可以打乱的，也可以是交错的；但为了研究起来方便，tensor 的定义强制要求了这个 "连续 $V^{\ast}$ 再连续 $V$" 的顺序

这个 $f$ 可以看做一个 $(r+s)$-tuple of vectors/covectors $(\mathbf{p}_1, \dots, \mathbf{p}_r, \mathbf{q}_1^T, \dots, \mathbf{q}_s^T)$，因为我们可以定义：

$$
f(\mathbf{w}_1^T, \dots, \mathbf{w}_r^T, \mathbf{v}_1, \dots, \mathbf{v}_s) = \prod_{i=1}^{r} \mathbf{w}_i^T \mathbf{p}_i \times \prod_{i=1}^{s} \mathbf{q}_i^T \mathbf{v}_i = a \in \mathbb{R}
$$

## 6. Tensor Product Operator

为了表示起来方便，我们引入 tensor product operator $\otimes$。它其实有两种应用场合：

1. 两个 tensor space over vector space $V$, $\Theta_1$ 和 $\Theta_2$ 的 tensor product $\Theta_1 \otimes \Theta_2$ 仍然是一个 tensor over vector space $V$
    - 考虑特殊情况：假设 $V$ 和 $W$ 都是 vector space (i.e. $T^1_0(V)$) over field $K$，那么 $V \otimes_K W$ 仍然是一个 vector space over field $K$
2. 两个 tensor, $t_1$ 和 $t_2$ 的 tensor product $t_1 \otimes t_2$ 仍然是一个 tensor
    - 考虑特殊情况：vector/covector/matrix 之间也可以有 $\otimes$ 操作

如果 $t_1 \in \Theta_1, t_2 \in \Theta_2$，那么 $t_1 \otimes t_2 \in \Theta_1 \otimes \Theta_2$。

我觉得暂时不要关注计算细节，先掌握大的计算原则比较重要。

### 6.1 用 $\otimes$ 表示 $L$

这个逻辑其实要绕一下：

- 假定有 $f: \underbrace{V^{\ast} \times \dots \times V^{\ast}}\_{r} \times \underbrace{V \times \dots \times V}\_{s} \to \mathbb{R}$，则 $f \in T\_{s}^{r}(V) = L^{r+s}(\underbrace{V^{\ast}, \dots, V^{\ast}}\_{r}, \underbrace{V, \dots, V}\_{s}; \mathbb{R})$
- 又:$f$ 可以看做一个 $(r+s)$-tuple of vectors/covectors $(\mathbf{p}_1, \dots, \mathbf{p}_r, \mathbf{q}_1^T, \dots, \mathbf{q}_s^T)$
- 我们可以写 $T\_{s}^{r}(V) = \underbrace{V \otimes \dots \otimes V}\_{r} \times \underbrace{V^{\ast} \otimes \dots \otimes V^{\ast}}\_{s}$
    - 注意这里 $V$、$V^{\ast}$ 的顺序和 $L$ 里是反的、和 $(r+s)$-tuple 是一致的

### 6.2 Rank of Tensor Product

基本原则 (参考 [StackExchange: Understanding the definition of tensors as multilinear maps, by celtschk](https://math.stackexchange.com/a/2141663))：

$$
T_{s}^{r}(V) \otimes T_{s'}^{r'}(V) \to T_{s+s'}^{r+r'}(V)
$$

$$
(f_1 \otimes f_2)(\underbrace{\kappa,\ldots,\lambda,\mu,\ldots,\nu}_{r+r'},\underbrace{u,\ldots,v,w,\ldots,z}_{s+s'}) = f_1(\underbrace{\kappa,\ldots,\lambda}_r,\underbrace{u,\ldots,v}_s) \cdot f_2(\underbrace{\mu,\ldots,\nu}_{r'},\underbrace{w,\ldots,z}_{s'})
$$

对 $T_n^n(V)$ 我们还可以进一步讨论一下：

- 按 [Wikipedia: Tensor product of linear maps](https://en.wikipedia.org/wiki/Tensor_product#Tensor_product_of_linear_maps) 的例子，matrix 对应 tensor，matrix 的 Kronecker Product 对应 tensor product。两个 $2 \times 2$ matrix (看作 $T^1_1(V)$) 的 Kronecker Product 是一个 $4 \times 4$ matrix，按道理它应该是一个 $T^2_2(V)$。按照这个逻辑展开，所有的 $T_n^n(V)$ 都是 matrix
- 至于这个 matrix 本身的维度，要从 $V$ 的维度说起:
    - 如果 $V \subseteq \mathbb{R}^m$，那么你的 $T^1_1(V)$ 应该是一个 $m \times m$ matrix (的集合)
        - 不可能不是 square matrix，因为 $\dim V = \dim V^{\ast} = m$
    - 按照 Kronecker Product 的算法，你的 $T^2_2(V)$ 应该是一个 $m^2 \times m^2$ matrix (的集合)


<!-- LaTeX 直接照搬自 wikipedia-->
$$
{\begin{bmatrix}a_{1,1}&a_{1,2}\\a_{2,1}&a_{2,2}\\\end{bmatrix}}\otimes {\begin{bmatrix}b_{1,1}&b_{1,2}\\b_{2,1}&b_{2,2}\\\end{bmatrix}}={\begin{bmatrix}a_{1,1}{\begin{bmatrix}b_{1,1}&b_{1,2}\\b_{2,1}&b_{2,2}\\\end{bmatrix}}&a_{1,2}{\begin{bmatrix}b_{1,1}&b_{1,2}\\b_{2,1}&b_{2,2}\\\end{bmatrix}}\\&\\a_{2,1}{\begin{bmatrix}b_{1,1}&b_{1,2}\\b_{2,1}&b_{2,2}\\\end{bmatrix}}&a_{2,2}{\begin{bmatrix}b_{1,1}&b_{1,2}\\b_{2,1}&b_{2,2}\\\end{bmatrix}}\\\end{bmatrix}}={\begin{bmatrix}a_{1,1}b_{1,1}&a_{1,1}b_{1,2}&a_{1,2}b_{1,1}&a_{1,2}b_{1,2}\\a_{1,1}b_{2,1}&a_{1,1}b_{2,2}&a_{1,2}b_{2,1}&a_{1,2}b_{2,2}\\a_{2,1}b_{1,1}&a_{2,1}b_{1,2}&a_{2,2}b_{1,1}&a_{2,2}b_{1,2}\\a_{2,1}b_{2,1}&a_{2,1}b_{2,2}&a_{2,2}b_{2,1}&a_{2,2}b_{2,2}\\\end{bmatrix}}.
$$
<!-- LaTeX 直接照搬自 wikipedia-->

这引出一个很重要的思想：**tensor 其实是 matrix of matrices**；或者更宽泛一点来讲：**tensor 是一个 meta-matrix，是一个 matrix of something，这个 something 是你可以自己定义的**。接着用上面那个例子：

- $t \in T^1_1(V)$ 是一个 $1 \times 1$ meta-matrix，它只有一个元素，但是这个元素是一个 $m \times m$ matrix，所以它整体上是一个 $m \times m$ matrix
    - 你也可以看成是 $m \times m$ 个 $s \in T^0_0(V)$
- $t \in T^2_2(V)$ 是一个 $m \times m$ meta-matrix，它的每个元素都是一个 $s \in T^1_1(V)$，所以它整体上是一个 $m^2 \times m^2$ matrix
    - 你也可以看成是 $m^2 \times m^2$ 个 $s' \in T^0_0(V)$
- 依此类推： 
    - $t \in T^n_n(V)$ 是一个 $m^{n-1} \times m^{n-1}$ meta-matrix，它的每个元素都是一个 $s \in T^1_1(V)$，所以它整体上是一个 $m^n \times m^n$ matrix
        - 你也可以看成是 $m^n \times m^n$ 个 $s' \in T^0_0(V)$
    - 或者我们写成 $T_n^n(V) = (T_1^1(V))^{\otimes n}$，其中 "$\otimes n$ 次方" 定义为：$V^{\otimes n} \equiv \underbrace{V \otimes \dots \otimes V}\_{n}$
        - 明显 $T_n^n(V) \otimes T_0^0(V) = T_n^n(V)$
- 考虑不规则的情况：
    - 若 $p > q$，则 $t \in T^p_q(V)$ 是一个 $m^{q} \times m^{q}$ meta-matrix，它的每个元素都是一个 $s \in T^{p-q}_0(V)$
    - 若 $p < q$，则 $t \in T^p_q(V)$ 是一个 $m^{p} \times m^{p}$ meta-matrix，它的每个元素都是一个 $s \in T^0_{q-p}(V)$

### 6.3 $T^0_0(V) = \mathbb{R}$ 的特殊性

严格来说，如果 $f_1: V \to X, f_2: W \to Y$，那么：

$$
f_1 \otimes f_2: V \otimes W \to X \otimes Y
$$

从函数的角度来看：

$$
(f_1 \otimes f_2)(\mathbf{v} \otimes \mathbf{w}) = f_1(\mathbf{v}) \otimes f_2(\mathbf{w})
$$

但是因为我们一般处理 $\mathbb{R}$，而 $\mathbb{R}$ 又是 $T^0_0(V)$，所以 $\mathbb{R} \otimes \mathbb{R} = T\_{0+0}^{0+0}(V) = \mathbb{R}$

所以我们一般的 $V \otimes W$ 的元素仍然是一个 $f: V \otimes W \to \mathbb{R}$

### 6.4 只有在把 tensor/tensor product 当作函数来做运算时你才会用到 Einstein summation convention

这里我就不展开了，可以参考：

- [http://rinterested.github.io/statistics/tensors3.html](http://rinterested.github.io/statistics/tensors3.html)
- [https://www.uio.no/studier/emner/matnat/math/nedlagte-emner/MAT-INF2360/v12/tensortheory.pdf](https://www.uio.no/studier/emner/matnat/math/nedlagte-emner/MAT-INF2360/v12/tensortheory.pdf)

最后说明一点：在工程应用中经常忽略掉运算结果里 basis 的 tensor product，只保留 tensor product 的分量。这和你写 vector 时只关注分量而忽略约定俗成的 basis $\mathbf{i}, \mathbf{j}, \mathbf{k}, \dots$ 道理是一样的。
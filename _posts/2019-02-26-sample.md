---
layout: post
title: "Terminology Recap: Sampling / Sample / Sample Space / Experiment / Statistical Model / Statistic / Estimator"
description: ""
category: Math
tags: []
---
{% include JB/setup %}

## 1. Sampling from a probability distribution

我觉得最好的解释在 [kccu@StackExchange: How does one formally define sampling from a probability distribution?](https://math.stackexchange.com/a/2336295):

> I have just described how to go from a random variable to its distribution function, but we can go the other way. Namely, given a distribution ($F_{X}$), we can sample a random variable from it, by which we mean we choose a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and a function $X:\Omega \to \mathbb{R}$ satisfying $\mathbb{P}(X \leq a)=F_{X}(a)$ for all $a \in \mathbb{R}$. It is not obvious that such a random variable must exist! But in fact one does provided you have a valid distribution function.

我们回头看一下没什么太大用的 sampling 的定义：[Wikipedia: Sampling (statistics)](https://en.wikipedia.org/wiki/Sampling_(statistics)):

> ... sampling is the selection of a subset (a statistical sample) of individuals from within a statistical population to estimate characteristics of the whole population. Statisticians attempt for the samples to represent the population in question. Two advantages of sampling are lower cost and faster data collection than measuring the entire population.

所以：

- 假设真实问题领域有一个 probability space $(\Omega, \mathcal{F}, \mathbb{P})$
    - 统计学上这个 outcome 的集合 $\Omega$ 在这里称为 **population** 
- 我们 somehow 从一个 distribution (搞不好还是从 sample 中估计出来的) 反向推出一个 probability space $(\Omega', \mathcal{F}', \mathbb{P}')$ (通过 sampling)
    - 这个 $\Omega' \subseteq \Omega$ 在这里称为一个 **statistical sample**
    - 搞不好这里还有个 "先有鸡还是先有蛋的问题"：你是先拿出一个 sample 来 estimate 一个 distribution，还是先拿出一个 distribution 反推出一个 sample？

## 2. Sample

虽然 sampling 的定义很清晰，但是很遗憾的是，一个 sample 可以指：

- 一个 random variable $X$ (来自 sampling)
    - [Terminology Recap: Random Variable / Distribution / PMF / PDF / Independence / Marginal Distribution / Joint Distribution / Conditional Random Variable](http://localhost:4000/math/2019/02/26/random-variable) 里我们已经见识到，一个 random variable vector (假设长度为 $m$) 也可以看做一个 random variable (引入 joint distribution)，所以一个 sample 也可能是一个 random variable vector，所以也可以理解为 $m$ 个 random variables，所以也可以理解为 $m$ 个 samples (:anger:)
- 一个 $\Omega' \subseteq \Omega$ (来自 sampling)
    - 假设 $\vert \Omega' \vert = m$，那么这个 $m$ 个 outcomes 又称为 $m$ 个 sample values
    - 所以一个 sample 也可以理解为 $m$ 个 sample values
        - 你要是理解成这 $m$ 个 sample values 是来自 $m$ 个 random variables，从而算是来自 $m$ 个 samples，我觉得你是把问题复杂化了。Don't do this.

## 3. Sample Space / Experiment

虽然看上出很自然，但是 sampling 出来的 $(\Omega', \mathcal{F}', \mathbb{P}')$ 中的 $\Omega'$ 并不叫 sample space。

而真正的 sample space 永远要和 experiment 联系起来。遗憾的是，experiment 没有 formal definition：[Wikipedia: Experiment (probability theory)](https://en.wikipedia.org/wiki/Experiment_(probability_theory))

> In probability theory, an **experiment** or **trial** is any procedure that can be infinitely repeated and has a well-defined set of possible outcomes, known as the sample space.

但是不要紧，你记住一点就行：**一个 experiment 一定对应一个 probability space $(\Omega'', \mathcal{F}'', \mathbb{P}'')$**:

- **然后这个 $\Omega''$ 就叫 sample space**，至于为什么这么叫，我倒是也想问问最初这么命名的人 (:rage:)
    - 发现了没？sample space 和 sampling 在定义上是没有直接关联的！老子信了你的邪！（当然你硬要从 sample 的定义上把这两者联系起来也是行得通的，但是，有帮助到你理解吗？）
- sample space $\Omega''$ 不一定是 population $\Omega$，但有可能是
    - sample space 小于 population 的例子
        - 比如 population 全国人口的身高
        - 然后 experiment 是在北京市抽查 100 人的身高，然后恰巧全国最高的人不在北京，你在北京怎么抽也不可能抽到这个最大的身高值
    - sample space 等于 population 的例子：
        - experiment 是 "roll a dice once"

## 4. Statistical Model

### 4.1 (预备知识) Mathematical Model

Mathematical model 是一个抽象概念：[Wikipedia: Mathematical model](https://en.wikipedia.org/wiki/Mathematical_model):

> A mathematical model is a description of a system using mathematical concepts and language.

- 那至于 [system 是什么](https://en.wikipedia.org/wiki/System)？这里就不展开了，也是个抽象的概念

Mathematical models are usually composed of **relationships** and **variables**. 

- Relationships can be described by operators, such as algebraic operators, functions, differential operators, etc. 
- Variables are abstractions of system parameters of interest, that can be quantified.

参照不同的标准，Mathematical model 可以分为以下几个大类：

- **Linear vs. nonlinear**
    - 判断标准是：operator 是否是 linear 的
- **Static vs. dynamic**
    - A dynamic model accounts for time-dependent changes in the state of the system。
        - 一般会用到 differential equations (微分方程) 或者 difference equations (差分方程)
            - 注：差分方程其实就是 递推关系 (recurrence relation)，类似 $a_{n+1} = f(a_n)$ 这种
    - A static (or steady-state) model calculates the system in equilibrium, and thus is time-invariant.
- **Explicit vs. implicit**
    - Explicit 指 "all of the input parameters of the overall model are known, and the output parameters can be calculated by a finite series of computations"
    - Implicit 指 "the output parameters are unknown, and the corresponding inputs must be solved for by an iterative procedure"
        - Newton's method 即是这类 iterative procedure 的一种
- **Discrete vs. continuous**
- **Deterministic vs. probabilistic (stochastic)**
    - A deterministic model is one in which every set of variable states is uniquely determined by parameters in the model and by sets of previous states of these variables; therefore, a deterministic model always performs the same way for a given set of initial conditions. 
    - In a stochastic model, randomness is present, and variable states are not described by unique values, but rather by probability distributions.
        - 下面所说的 probability model 和 statistical model 都属于 stochastic models 的大类
- **Deductive vs. inductive**
    - A deductive model is a logical structure based on a theory. 
    - An inductive model arises from empirical findings and generalization from them. 

### 4.２ (预备知识) Probability Model

废话少说：A probability model is defined by a probability space $(\Omega, \mathcal{F}, \mathbb{P})$. Period.

- 注：$\Omega$ 是 sample space

### 4.３ Statistical Model 

根据 [MIT 18.650 Statistics for Applications](https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/MIT18_650F16_Parametric_Inf.pdf):

Let the observed outcome of a statistical experiment be a sample of $m$ i.i.d. random variables $X_1, \dots, X_m$ in some measurable space $E$ (usually $E \subseteq \mathbb{R}$) and denote by $\mathbb{P}^\ast$ their common distribution. A statistical model associated to that statistical experiment is a pair $(E, \mathcal{P})$, where

- $E$ is the sampel space,
- $\mathcal{P} = \lbrace \mathbb{P}_{\theta} \mid \theta \in \Theta \rbrace$ is a family of probabilty measures (i.e. distributions) on $E$, and
- $\Theta$ is the parameter set.

$\mathcal{P}$ 举个例子："不同参数的 Gaussian distributions"：$\mathcal{P} = \lbrace \mathbb{P}\_{\mathcal{N}(\sigma, \mu^2)} \mid (\sigma, \mu) \in \mathbb{R}^2 \rbrace$. 

#### 4.3.1 Well-specified Statistical Model

A statistical model is said to be **well-specified** if $\exists \theta^\ast \in \Theta$ such that $\mathbb{P}_{\theta^\ast} = \mathbb{P}^\ast$.

A statistical model is said to be **misspecified** if $\not \exists \theta^\ast \in \Theta$ such that $\mathbb{P}_{\theta^\ast} = \mathbb{P}^\ast$.

[What does it mean for a probability model to be “well-specified” or “misspecified”?](https://math.stackexchange.com/a/2175180):

> _Well-specified_ means that the class of distribution $\mathcal{C}$ you are **assuming** for your modeling actually contains the unknown probability distribution $p$ from where the sample is drawn.

> _Misspecified_ means, on the other hand, that $\mathcal{C}$ does not contain $p$. You made a modeling assumption, and it is not perfect: for instance, you assume your sample is Gaussian, but (maybe due to noise, or just inherently) it is not actually originating from any Gaussian distribution.

一般情况下，我们都是 **assume** that our statistical model is well-specified.

This particular $\theta^\ast$ is called the **true parameter**, and is unknown: The aim of the statistical experiment is to estimate $\theta^\ast$, or check its properties when they have a special meaning (比如 $\theta^\ast > 2$ 时 $\mathbb{P}_{\theta^\ast} = \mathbb{P}^\ast$ 有什么特点)

#### 4.3.2 Identifiable Statistical Model

We say that statistical model $(E, \mathcal{P})$ is **identifiable** if the mapping $\theta \mapsto \mathbb{P}_{\theta}$ is injective, i.e.

$$
\theta = \theta' \Rightarrow \mathbb{P}_{\theta} = \mathbb{P}_{\theta'}
$$

此时 any parameter $\theta$ can be called **identified**.

#### 4.3.3 Dimension of a Statistical Model

如果有 $\Theta \subseteq \mathbb{R}^k$，我们称 $k$ 为 **dimension of the statistical model**:

- The model is said to be **parametric** if it has a finite dimension
- The model is said to be **nonparametric** if it has a infinite dimension

注：dimension 不是 statistical model 专有的，其他涉及 parameter 的 mathematical model 应该都有这个概念。参考 [Parametric vs. non-parametric models](/math/2015/06/20/parametric-vs-non-parametric-models)


## 5. Statistic / Estimator

Given an observed sample $X_1, \dots, X_m$ and a statistical model $(E, \mathcal{P})$, we define:

- A **statistic** is any [measurable function](/math/2019/02/26/random-variable) of the sample
- An **estimator** of $\theta^\ast$, denoted by $\hat \Theta_m$, is any statistic whose expression does not depend on $\theta^\ast$
    - 比如 $\hat \Theta_m = f(X_1, \dots, X_m)$
- An **estimate** of $\theta^\ast$ is denoted by $\hat \theta_m$
    - 比如 $\hat \theta_m = f(x_1, \dots, x_m)$
    - 这里又涉及到类似 "一个 random variable $X=3$ 是什么意思?" 的问题。我们在根据 $\hat \Theta_m$ 计算 $\hat \theta_m$ 的时候的确就是把 $X_i$ 替换成 $x_i$ 而已，但这并不是 $X_i = x_i$ 的意思，也不是说要用 $X_i(x_i)$ 这个值去算，而是模拟了一个 $X_i(\overset{?}{\cdot}) = x_i$ 的过程，至于这个 $\overset{?}{\cdot}$ 是多少这里我们并不关心

注意下逻辑关系：

- estimand $\theta^\ast$ = the parameter of interest (to be estimated)
- estimator $\hat \Theta_m$ = a function (of estimating)
- estimate $\hat \theta_m$ = a value (of estimating)

从本质上来说，statistic/estimator 是一个 function，也是个 random variable，estimate 是一个值，但遗憾的是：

- 因为 statistic 没有一个名词去表示它的具体值，所以 statistic 有时表示函数 (or random variable)，有时表示一个具体值
- estimator 一定是函数 (or random variable)，但有时有的人就是要叫它 estimate，而且就是不愿意用大写字母表示 (:anger:)，所以你看到 "an estimate $\hat \theta_m$" 这样的描述，你需要根据上下文来判断它到底说的是 "an estimator $\hat \Theta_m$" 还是 "an estimate $\hat \theta_m$" (:expressionless:)

对 parameter 的 estimation 我们称为 point estimation (视 $X_1, \dots, X_m$ 为 $m$ 个 points)，我们可以把它引申到对 function 的 estimation (也称 function approximation)。假设 estimand 是 $g^\ast$，estimate 是 $\hat g_m$，我们也可以计算它们的 error、bias 之类的。**A function estimator can be seen as simply a point estimator in function space**.

### 5.1 Bias

The **bias** of an estimator $\hat \Theta_m$ is defined as:

$$
\operatorname{bias}(\hat \Theta_m) = \mathbb{E}[\hat \Theta_m] - \theta^\ast
$$

An estimator $\hat \Theta_m$ is said to be **unbiased** if $\operatorname{bias}(\hat \Theta_m) = 0$, which implies that $\mathbb{E}[\hat \Theta_m] = \theta^\ast$. 

An estimator $\hat \Theta_m$ is said to be **asymptotically unbiased** if $\underset{m \to \infty}{\lim} \operatorname{bias}(\hat \Theta_m) = 0$, which implies that $\underset{m \to \infty}{\lim} \mathbb{E}[\hat \Theta_m] = \theta^\ast$.

### 5.2 Variance / Standard Deviation / Standard Error (to its Mean)

对任意一个 random variable $X$，它的 **variance** 为：

$$
\operatorname{Var}(X) = \mathbb{E} \big[(X - \mathbb{E}[X])^2\big]
$$

**Standard deviation** 为：

$$
\sigma(X) = \sqrt{\operatorname{Var}(X)}
$$

**Standard error (to its mean)** 为：

$$
\operatorname{SEM}(X) = \sqrt{\frac{\operatorname{Var}(X)}{m}} = \frac{\sigma(X)}{\sqrt{m}}
$$

- $m$ is the size of the sample (number of observations).

你把 $X$ 换成 $\hat \Theta_m$，就能得到 estimator 的 variance、standard deviation、standard error (to its mean).

需要注意的一点是：

- $\operatorname{bias}(\hat \Theta_m)$ 有涉及 $\theta^\ast$，所以它研究的是 "**$\Theta_m$ 与 $\theta^\ast$ 之间的关系**"
- 而 $\operatorname{Var}(\hat \Theta_m)$, $\sigma(\hat \Theta_m)$, $\operatorname{SEM}(\hat \Theta_m)$ 和 $\theta^\ast$ 无关，所以它们表示的是 "**$\Theta_m$ 本身的性质**"

### 5.3 Error (to the Parameter) / Mean Square Error (to the Parameter) / (Quadratic) Risk

The **error** of an estimator $\hat \Theta_m$ to the parameter $\theta^\ast$ is defined as:

$$
e(\hat \Theta_m) = \hat \Theta_m - \theta^\ast
$$

The **mean square error** of an estimator $\hat \Theta_m$ to the parameter $\theta^\ast$ is defined as:

$$
\operatorname{MSE}(\hat \Theta_m) = \mathbb{E} \big[ e(\hat \Theta_m)^2 \big] = \mathbb{E} \big[ (\hat \Theta_m - \theta^\ast)^2 \big]
$$

The **(quadratic) risk** of an estimator $\hat \Theta_m$ is defined as:

$$
\operatorname{risk}(\hat \Theta_m) = \mathbb{E} \big[\vert \hat \Theta_m - \theta^\ast \vert^2 \big]
$$

- 明显，$\operatorname{risk}(\hat \Theta_m) = \operatorname{MSE}(\hat \Theta_m)$

If $\theta^\ast \in \mathbb{R}$ (一维参数), then:

$$
\operatorname{risk} = \operatorname{MSE} = \operatorname{bias}^2 + \operatorname{Var}
$$

### 5.4 Consistency

Let $X_1, \dots, X_m$ be a sample sharing a common distribution $\mathbb{P}$ from a statistical model $(E, \mathcal{P})$ (which implies $\mathbb{P} \in \mathcal{P}$), and $\hat \Theta_m = f(X_1, \dots, X_m)$ be an estimator of $\theta^\ast$.

Estimator $\hat \Theta_m$ is said to be **(weakly) consistent** if it converges to $\theta^\ast$ in probability, i.e.:

$$
\underset{m \to \infty}{p\lim} \hat \Theta_m = \theta^\ast
$$

which means $\forall \epsilon > 0$:

$$
\underset{m \to \infty}{\lim} \mathbb{P}(\lvert \hat \Theta_m - \theta^\ast \rvert > \epsilon) = 0
$$

- 我们也可以把 convergence in probability 写作 $\hat \Theta_m \overset{\mathbb{P}}{\rightarrow} \theta^\ast$

Estimator $\hat \Theta_m$ is said to be **mean square consistent** if $\underset{m \to \infty}{\lim} \operatorname{MSE}(\hat \Theta_m) = 0$

Estimator $\hat \Theta_m$ is said to be **$L_2$ consistent** if $\lVert \hat \Theta_m - \theta^\ast \rVert_2 \overset{\mathbb{P}}{\rightarrow} 0$

- 这么一来，weak consistency 也可以看做是 **$L_1$ consistency**

Estimator $\hat \Theta_m$ is said to be **strongly consistent** if it converges to $\theta^\ast$ almost surely, i.e. $\forall \epsilon > 0$:

$$
\mathbb{P} \big ( \underset{m \to \infty}{\lim} \lvert \hat \Theta_m - \theta^\ast \rvert \leq \epsilon \big ) = 1
$$

- 我们也可以把 almost sure convergence 写作 $\hat \Theta_m \overset{\text{a.s.}}{\rightarrow} \theta^\ast$

注意：

- $L_1$ consistency $\Leftrightarrow$ weak consistency
- mean square consistency $\Rightarrow$ weak consistency
- $L_2$ consistency $\Rightarrow$ weak consistency
- strong consistency $\Rightarrow$ weak consistency
- 但目前 mean square consistency、$L_2$ consistency 和 strong consistency 这三者的强弱关系我还不清楚

## 6. Empirical Distribution Function $\hat{F}\_{\mathbb{P}\_{X}}$

Let $X_1, \dots, X_m$ be i.i.d. real random variables with the common distribution function $F\_{\mathbb{P}\_{X}}$. Then the empirical distribution function (EDF) is defined as:

$$
\hat{F}_{m}(a) = \hat{F}_{\mathbb{P}_{X}}(a) = \frac{1}{m} \sum_{i=1}^{m} \mathbf{1}_{X_{i} \leq a}
$$

where $\mathbf{1}_{A}$ is the indicator of event $A$.

- 注意 $F\_{\mathbb{P}\_{X}}(a) = \mathbb{P}(X \leq a)$

For a fixed $a$, the indicator $\mathbf{1}\_{X\_{i} \leq a}$ is a Bernoulli random variable with parameter $p = F\_{\mathbb{P}\_{X}}(a)$; hence $m\hat{F}\_{m}(a)$ is a binomial random variable with mean $mF\_{\mathbb{P}\_{X}}(a)$ and variance $m F\_{\mathbb{P}\_{X}}(a) \big( 1 − F\_{\mathbb{P}\_{X}}(a) \big)$. This implies that $\hat{F}_{m}(a)$ is an unbiased estimator for $F\_{\mathbb{P}\_{X}}(a)$.

另外 $\forall a$, $\hat{F}_{m}(a) \overset{\text{a.s.}}{\rightarrow} F\_{\mathbb{P}\_{X}}(a)$

## 7. Connections between Estimation and Machine Learning

### 7.1 Unsupervised / Supervised Learning

假设 population $X^\ast$ 的 true distrbution 是 $\mathbb{P}^\ast$，我们 observe 到一个 random vector $X = (X_1, \dots, X_m)$，那么 (roughly speaking) unsupervised learning 可以理解为用 $\hat{\mathbb{P}}_{X}$ 去 estimate $\mathbb{P}^\ast$，i.e.:

$$
\hat{\mathbb{P}}_{X} = \hat{\mathbb{P}}_m \overset{?}{\to} \mathbb{P}^\ast
$$

- unsupervised learning 也有可能只 estimate $\mathbb{P}^\ast$ 的部分性质而不是一定要 estimate $\mathbb{P}^\ast$.

假设 population $Y^\ast \vert X^\ast$ 的 true distrbution 是 $\mathbb{P}^\ast$，我们 observe 到一个 random vector $X = (X_1, \dots, X_m)$ 以及 associated value vector $Y = {Y_1, \dots, Y_m}$，那么 (roughly speaking) supervised learning 可以理解为用 $\hat{\mathbb{P}}_{Y \mid X}$ 去 estimate $\mathbb{P}^\ast$，i.e.:

$$
\hat{\mathbb{P}}_{Y \mid X} = \hat{\mathbb{P}}_m \overset{?}{\to} \mathbb{P}^\ast
$$

- 一旦我们 assume 了 statistical model，那么 "estimating distribuition $\mathbb{P}^\ast$ 的过程" 就可以转化成 "estimating parameter $\theta^\ast$ 的过程"

### 7.2 Training Error / Test Error / Underfitting / Overfitting

我们用 design matrix 来描述 dataset，i.e. $m$ rows (examples; random variables) by $n$ columns (features). 对单个特定的问题，我们称所有的 datasets 都是通过 **data generating process**、依据一个统一的 **data generating distribution** $\mathbb{P}_\text{data}$ (i.e. $\mathbb{P}^\ast$) 来生成的。然后我们假设，无论你如何划分 training dataset $X^{\text{train}}$ 和 test dataset $X^{\text{test}}$，都有：

- $X_1^\text{train}, \dots, X_m^\text{train}$ are i.i.d (by distribution $\mathbb{P}_\text{data}$)
- $X_1^\text{test}, \dots, X_{m'}^\text{test}$ are i.i.d (by distribution $\mathbb{P}_\text{data}$)

我们在 evaluating machine learning model 的时候一般会选取一个 "error measure"，比如说 $\operatorname{MSE}$，那在 training dataset 上就是 training error，在 test dataset 上就是 test error。如果我们是先确定了一个 $\theta^\dagger$ (不一定要是 $\mathbb{P}\_\text{data}$)，然后根据 $\mathbb{P}\_{\theta^\dagger}$ 来 sample 出一个 training dataset 和 test dataset，那么 expected training error 应该等于 expected test error，比如:

$$
\operatorname{MSE}(\hat{\theta}_{X^\text{train}}) = \operatorname{MSE}(\hat{\theta}_{X^\text{test}}) = \operatorname{MSE}(\theta^\dagger)
$$

- 注意我们这里用的是 $\operatorname{MSE}$ of an estimate 而不是 of an estimator

但是 learning 的过程不是这样的，而是先 estimate $\hat{\Theta}_{X^\text{train}}$，再把 $X^\text{test}$ 的值代进去算的：

$$
\begin{aligned}
\text{training error} &= \operatorname{MSE} \big( \hat{\Theta}_{X^\text{train}}(x^\text{train}) \big) = \operatorname{MSE}(\hat{\theta}_{X^\text{train}}) \\
\text{test error} &= \operatorname{MSE} \big( \hat{\Theta}_{X^\text{train}}(x^\text{test}) \big) \neq \operatorname{MSE}(\hat{\theta}_{X^\text{test}})
\end{aligned}
$$

- 这个 $\hat{\Theta}_{X^\text{train}}(x^\text{test})$ 的矛盾就是 **generalization** error 的根源
    - 你可能会注意到这里有个 dimension 的问题：若 $\hat{\Theta}\_{X^\text{train}} = f(X\_1^\text{train}, \dots, X\_m^\text{train})$ 的话，你 $X^\text{test} = (X\_1^\text{test}, \dots, X\_{m'}^\text{test})$ 有 $m'$ 项，如何代入 $f$ 计算？秘诀就是我们可以规定 $f$ 是单个 random vector 的函数，而不是固定数量的 random variables 的函数
- 然后你计算的过程决定了：$\hat{\Theta}\_{X^\text{train}}$ 只会 minimize $\operatorname{MSE}$ on $x^\text{train}$，所以一般有 $\text{training error} \leq \text{test error}$ ([proof skeleton](http://www.stat.cmu.edu/~ryantibs/advmethods/homeworks/homework02.pdf))
    - **Underfitting:** $\text{training error}$ high, $\text{test error}$ high
    - **Overfitting:** $\text{training error}$ low, $\text{test error}$ high
    - **Good fit:** $\text{training error}$ low, $\text{test error}$ slightly higher
    - Unknown fit: $\text{training error}$ high, $\text{test error}$ low
- 出现 overfitting 的一个原因可能是，我们 minimize 的 $\operatorname{MSE}$ 并不是 $\hat{\Theta}\_{X^\text{train}}$ 和 $\theta^\ast$ 之间的 error，因为 $\theta^\ast$ 我们并不知道，所以我觉得在实际计算时，我们用的是 empirical distribution 的参数 $\theta^\ast\_{X^\text{train}} := \theta^\mathbf{1}\_{X^\text{train}}$，所以最终的关系可能是：

$$
\hat{\Theta}_{X^\text{train}} \hat= \theta^{\mathbf{1}}_{X^\text{train}} \overset{?}{\rightarrow} \theta^\ast
$$

- 所以一定程度以后，越逼近 $\theta^{\mathbf{1}}_{X^\text{train}}$ 可能离 $\theta^\ast$ 越远

### 7.4 Bias-Variance Tradeoff

我这里只是想说一下这个 tradeoff 和这个式子其实是没什么关系的：

$$
\operatorname{risk} = \operatorname{MSE} = \operatorname{bias}^2 + \operatorname{Var}
$$

因为 bias-variance tradeoff 实际是在权衡不同类型的 models 之间的利弊 (high-variance-low-bias vs low-variance-high-bias)，那 model 变了，estimator 自然就变了，$\operatorname{MSE}$ 自然也变了；所以并不是说你可以维持一个具体的 $\operatorname{MSE}$ 不变，然后 somehow 让 $\operatorname{bias}$ 和 $\operatorname{Var}$ negatively correlated 地变化。

那为什么是 high-variance-low-bias vs low-variance-high-bias？因为这俩是常态，high-variance-high-bias 这属于 model 本身有问题，low-variance-low-bias 这已经是最优解了，还 tradeoff 个啥。

我有写过一篇 [Hypothesis Space / Underfitting / Overfitting / Bias / Variance](/machine-learning/2018/05/15/hypothesis-space-underfitting-overfitting-bias-variance) 可以参考。

[普遍的规律](https://elitedatascience.com/bias-variance-tradeoff)是：

- conservative (less complex) models 一般 low-variance-high-bias，有 underfitting 倾向
- flexible (more complex) models 一般 high-variance-low-bias，有 overfitting 倾向

### 7.5 Bayes Error

我们常见有假设 linear model $Y = \mathbf w X + \epsilon$ 的做法，这是假设了 even in true distribution $\mathbb{P}^\ast$ there may still be some noise. 这个 error 我们称为 Bayes error。更多内容参考：

- [What is Bayes Error in machine learning?](https://stats.stackexchange.com/questions/302900/what-is-bayes-error-in-machine-learning)
- [Bayes classifier and Bayes error](https://www.cs.helsinki.fi/u/jkivinen/opetus/iml/2013/Bayes.pdf)

有时为了进一步研究的方便，我们假设 $\epsilon \sim \mathcal{N}(0, 1)$

<!--
https://math.stackexchange.com/a/2336295

https://stats.stackexchange.com/questions/85426/is-test-statistic-a-value-or-a-random-variable
https://stats.stackexchange.com/questions/317541/why-is-an-estimator-considered-a-random-variable

https://stats.stackexchange.com/questions/358342/what-the-relation-between-a-random-variable-and-a-sample-or-dataset-in-machine

https://math.stackexchange.com/questions/1251393/when-is-the-maximum-likelihood-estimator-measurable

https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/MIT18_650F16_Parametric_Inf.pdf
http://www.stats.ox.ac.uk/~steffen/teaching/bs2siMT04/si2c.pdf
https://ocw.mit.edu/courses/mathematics/18-655-mathematical-statistics-spring-2016/lecture-notes/MIT18_655S16_LecNote16.pdf
-->